### 4.2.8. 다중 데이터 센터 설계 `Enterprise Edition`
> 이 장에서는 자신의 다중 데이터 생산 환경을 위한 빌딩 블록의 역할을 할 수 있는 다중 데이터 센터 배포를 위한 일반적인 패턴에 대해 설명합니다.

이 장에서는 다중 데이터 센터 환경을 위한 Causal 클러스터를 설계할 때 고려해야 할 여러 고려 사항을 설명하는 일련의 예제를 기반으로 합니다. 우리는 일반적인 다중 데이터 센터 배포 시나리오의 약점과 이점을 이해하게 될 것입니다. 각 시나리오는 명확성을 위해 높은 아키텍처 수준에서 제시됩니다. 다음 장에서는 이러한 배포가 어떻게 구성되는지 더 자세히 설명될 것입니다.

#### 4.2.8.1. 코어 서버 배포 시나리오
각 DC에 동일한 수와 종류의 인스턴스를 배포하는 개념적으로 가장 단순한 다중 데이터 센터 시나리오부터 시작하겠습니다. 이것은 각 데이터 센터가 다른 데이터 센터와 동일하기 때문에 *동종의* 배포입니다.

**예제 4.9. 동종의 3개 데이터 센터 구축**

----------------------------

**그림 4.9. 각각에 하나의 코어 인스턴스가 있는 세 개의 데이터 센터에서의 동종 배포**
![3-dc-homogeneous](./3-dc-homogeneous.png)

위의 다이어그램에서 우리는 3개의 데이터 센터를 가지며, 각각 동일하게 하나의 코어 서버와 적은 수의 Read Replica를 갖추고 있습니다.

Raft는 인스턴스의 대다수가 안전하게 커밋되기 전에 쓰기를 승인하도록 요구하기 때문에, 이 패턴의 커밋 경로 대기 시간에는 가장 빠른 데이터 센터 두 개만 관련됩니다. 따라서 이 설정에 대한 커밋 비용은 다음의 두 WAN 메시지입니다: 트랜잭션을 보내기 위한 것과 ACK 메시지. 장애가 아닌 경우에는 다른 데이터 센터가 크게 지연되지 않을 것이며 트랜잭션도 적용될 것입니다.

각 데이터 센터 내에서 더 많은 코어 인스턴스를 추가하여 시스템 수준의 중복성을 높일 수 있습니다. 예를 들어 각 데이터 센터에 두 대 이상의 시스템을 추가하여 클러스터 또는 단일 데이터 센터 전체에서 최대 4대의 시스템이 자연스럽게 손실되는 것을 허용할 수 있습니다.

**그림 4.10. 각각 세 개의 코어 인스턴스가 있는 세 개의 데이터 센터에 걸치는 동종의 배포**
![3-dc-homogeneous-3-core](./3-dc-homogeneous-3-core.png)

이 배치 패턴의 장단점을 요약하면 다음과 같습니다:
* 우리는 가용성을 유지하면서 전체 데이터 센터를 잃어버릴 수 있으며, 각 데이터 센터의 시스템 수에 따라 현재 데이터 센터에 관계없이 개별 서버의 손실을 허용 할 수 있습니다.
* 트랜잭션에 대한 커밋 경로가 짧으며, 두 개의 WAN 메시지만 교환됩니다.
* 대다수 데이터 센터의 손실은 [복구해야](./disaster-recovery.md) 하지만, 데이터 센터 중 어느 것이 손실되었는지에 관계없이 운영 절차는 동일합니다.

------------------------------------------------------------------------------------------------------------

[다중 데이터 센터 구성에 대한 장](./configuration.md)에서 볼 수 있듯이, Read Replica는 캐치 대기 시간을 최소화하기 위해 데이터 센터 로컬 코어 서버에서 따라 잡을 수 있도록 편향될 수 있습니다. 또한 데이터 센터 로컬 클라이언트 애플리케이션은 토폴로지 인접성과 확장을 위해 동일한 Read Replica로 라우팅 될 가능성이 높습니다. 더 자세한 내용은 [다중 데이터 센터 로드 밸런싱에 관한 장](./load-balancing.md)을 참조하십시오.

두 데이터 센터의 경우, 첫 번째 본능은 운영 일관성을 위해 사용 가능한 서버의 균형을 맞추는 것입니다. 각각 2개의 코어 인스턴스가 있는 두 데이터 센터에 걸친 동종 배포의 예가 아래 다이어그램에 나와 있습니다:

**예제 4.10. 동종의 두 데이터 센터 배포**

-----------------------------

**그림 4.11. 두 데이터 센터를 걸치는 동종 배포**

![2-dc-homogeneous-2-core](./2-dc-homogeneous-2-core.png)

이 구성의 문제점은 구조적으로 단순하지만, 대다수 합의에 기반한 Raft 프로토콜의 장점에 맞지 않는다는 것입니다. 장애가 아닌 경우, 대다수의 커밋은 비 로컬 데이터 센터 인스턴스로부터 적어도 하나의 응답을 함축하기 때문에 어떤 트랜잭션을 커밋하기 위해서는 두 개의 WAN 메시지가 발생합니다. 더 나쁜 것은 데이터 센터 중 하나가 손실되면 과반수를 달성하는 것은 불가능하기 때문에 클러스터가 읽기 전용이 된다는 것입니다.

-------------------------------------------------------------------

위의 예에서 볼 수 있듯이, 두 데이터 센터를 통한 동종 배포는 Causal 클러스터링의 장점을 충분히 활용하지 못합니다. 그러나 전체 데이터 센터 손실의 경우, 전체 Raft 로그가 두 데이터 센터 모두에 존재하도록 보장합니다.

데이터 센터를 중심으로 코어 서버를 분산시키는 것과는 달리, 모든 서버는 하나로 호스팅해야 합니다. 이것은 기술 또는 관리상의 이유로 발생할 수 있지만, 어느 쪽이든 쓰기에 대한 LAN 커밋 대기 시간의 장점이 있습니다.

코어 서버들이 함께 배치되는 동안, 팬 아웃 스케일링 (fan-out scaling)을 가능하게 하기 위해 클라이언트 애플리케이션 근처에 Read Replica를 분산시킵니다.

**예제 4.11. 데이터 센터별로 분리된 코어 서버 및 Read Replica**

-----------------------------------------

아래 다이어그램은 하나의 데이터 센터에 쓰기를, 모두에게 읽기를 지시하는 이기종 배치의 예제를 보여줍니다. 이 패턴은 지리적 복제로 인해 데이터의 높은 생존 가능성을 제공합니다. 또한 클라이언트 애플리케이션의 인접성도 제공합니다. 그러나 코어 서버 데이터 센터가 손실되면, 즉시 복구를 시작하고 나머지 Read Replica 데이터 센터 중 하나를 새 코어 클러스터로 전환해야 합니다.

데이터 센터 1의 손실 이전에 확인된 트랜잭션을 모두 수신한 Read Replica가 없을 수 있습니다. 이것은 지리 복제에 편리한 패턴이지만, 그것의 의미는 최선의 노력입니다. 클러스터 설계자는 복구 전략을 결정할 때 이 부분을 고려해야만 합니다.

그림 4.12. 코어 클러스터에서 Read Replica를 분리하는 이기종 배포
![3-dc-heterogeneous](./3-dc-heterogeneous.png)

이 접근 방식으로의 운영상 조정은 데이터 센터 2및 3의 코어 서버를 복구의 시작 지점으로 호스팅 하는 것입니다. 정상적인 작동 중에 이러한 여분의 코어 서버는 [*팔로워 전용*](../create-a-new-causal-cluster.md#4235-팔로워-전용-인스턴스의-바이어스-클러스터-리더십)으로 구성되어야 합니다. 데이터 센터 1이 손실되면, 이 코어 서버 중 하나를 사용하여 새로운 코어 클러스터를 빠르게 부트 스트랩하고 전체 서비스로 신속하게 복귀할 수 있습니다.

이 배포 패턴의 강점을 요약하면 다음과 같습니다:
* 
